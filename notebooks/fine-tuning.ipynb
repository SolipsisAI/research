{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc19e10-b8f3-4b0e-8e0e-6a3d74c79d82",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT\n",
    "\n",
    "https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/#data-loading-and-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5342b571-99e6-4257-9052-9a2eeb9db178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50890486-20bc-4265-b21b-2bc52142159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, re, pandas as pd, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from datasets import Dataset, list_metrics, load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d1459b-99ae-4d54-ae62-1ff1f60666ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"gpt2\"\n",
    "#model_cls = GPT2LMHeadModel\n",
    "#tokenizer_cls = GPT2Tokenizer\n",
    "\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"\n",
    "model_cls = AutoModelForCausalLM\n",
    "tokenizer_cls = AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1722bf0b-9462-4fbe-a369-ab4e3ac13c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    cuda: True\n",
      "    current_device: 0\n",
      "    device_count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev) \n",
    "\n",
    "print(f\"\"\"\n",
    "    cuda: {torch.cuda.is_available()}\n",
    "    current_device: {torch.cuda.current_device()}\n",
    "    device_count: {torch.cuda.device_count()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e5c7d-99cd-4fb0-b92c-c06fc8d31739",
   "metadata": {},
   "source": [
    "## Load Data, Tokenizer, and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae2ca3-2502-4c89-b8f7-d2438099d78b",
   "metadata": {},
   "source": [
    "### Data Loading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6348c3-7b02-48e6-bd03-8b54763c722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32080</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>What're the odds, right? But yes, we, uh...know each other.  And he's also here in Boston.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32081</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Right. Geeze. I...did not expect to ever hear from him again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32082</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>...Surprise?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32083</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>So, uh, would you want to talk to him? Because this is all stuff he really should get the chance to ask you about directly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32084</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Um...sure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32085</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Ok, good. Um, we should probably go. But, uh, I'll see you tomorrow?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32086</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[_as she talks she takes the book off the table and carefully tucks it away in her bag_]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32087</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Um, yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32088</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32089</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[_she will leave with the others_]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       character  \\\n",
       "32080  Charlotte   \n",
       "32081       Toby   \n",
       "32082  Charlotte   \n",
       "32083  Charlotte   \n",
       "32084       Toby   \n",
       "32085  Charlotte   \n",
       "32086  Charlotte   \n",
       "32087       Toby   \n",
       "32088       Toby   \n",
       "32089  Charlotte   \n",
       "\n",
       "                                                                                                                              text  \n",
       "32080                                   What're the odds, right? But yes, we, uh...know each other.  And he's also here in Boston.  \n",
       "32081                                                                Right. Geeze. I...did not expect to ever hear from him again.  \n",
       "32082                                                                                                                 ...Surprise?  \n",
       "32083  So, uh, would you want to talk to him? Because this is all stuff he really should get the chance to ask you about directly.  \n",
       "32084                                                                                                                   Um...sure.  \n",
       "32085                                                         Ok, good. Um, we should probably go. But, uh, I'll see you tomorrow?  \n",
       "32086                                     [_as she talks she takes the book off the table and carefully tucks it away in her bag_]  \n",
       "32087                                                                                                                    Um, yeah.  \n",
       "32088                                                                                                                    Tomorrow.  \n",
       "32089                                                                                           [_she will leave with the others_]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"../data/processed.csv\"\n",
    "df = pd.read_csv(filepath, encoding=\"utf-8\", usecols=[\"character\", \"content\"]).rename(columns={\"content\": \"text\"})\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f9585-6e12-4bf1-990f-40aeb39f5952",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b09bb6c-1983-4e87-b805-3dab99ae9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model_cls.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502b3b5d-d0b6-47f9-9424-a0a646a95ba0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ModuleUtilsMixin.num_parameters of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e23c7b-052d-411c-ac45-66bfcf9540f0",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ff1d2c-2dfd-4f29-939e-bf4f299a3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = tokenizer_cls.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138c94af-7007-4dbb-afe4-59eeeb05d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab_size: {base_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfce691-d927-41cd-95bd-db815dfb6c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = base_tokenizer.get_vocab()\n",
    "vocabulary[\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affbeb1a-faed-4cae-9c70-be9de39dc1e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode\n",
    "text = \"Hi, how are you?\"\n",
    "base_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fb3dea-2b04-482d-913b-2289a6089cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17250,    11,   703,   389,   345,    30]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids = base_tokenizer.encode(text, return_tensors = 'pt')\n",
    "text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9ad140-f825-499d-b16e-03e7ec09cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ae1eac-5b6b-4435-82f4-a137481220a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a1ff8-a41b-40f1-a5a9-4fb6211b04a9",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944c7d1-207e-4a11-b959-54a18f3bd1a6",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637843c6-cac9-44dc-bc6c-866c7a88d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ejemplo de generación de texto\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 100,\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcadd20-f8ff-420f-a77f-59003a8b8112",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1501ac8-53f3-442c-994b-b37d2c427c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n",
      "1: Hi, how are you? Good morning!\n",
      "\n",
      "2: Hi, how are you?'s hello\n",
      "\n",
      "3: Hi, how are you? Good Morning everyone!\n",
      "\n",
      "4: Hi, how are you? Good morning everyone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    num_return_sequences= 5,\n",
    "    early_stopping=True \n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873ed406-4987-4167-bf11-259abcadc2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n",
      "1: Hi, how are you? Good morning!\n",
      "\n",
      "2: Hi, how are you?'s hello\n",
      "\n",
      "3: Hi, how are you? Good Morning everyone!\n",
      "\n",
      "4: Hi, how are you? Good morning everyone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences= 5,\n",
    "    early_stopping=True \n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "854fe608-e368-4bd7-b428-4efa2d6086dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning darling.\n",
      "\n",
      "1: Hi, how are you?.\n",
      "\n",
      "2: Hi, how are you?'s Hello.\n",
      "\n",
      "3: Hi, how are you?'s hi... Cx\n",
      "\n",
      "4: Hi, how are you? Good morning bass.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02e0afa2-fa35-493b-b473-897388a3665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? I see you everywhere in r all.\n",
      "\n",
      "1: Hi, how are you? Good Morning, all? Good Morning!\n",
      "\n",
      "2: Hi, how are you?\n",
      "\n",
      "3: Hi, how are you?'s hi.\n",
      "\n",
      "4: Hi, how are you? How are we both still in school?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613fef48-67a7-4303-8e17-eb82107b0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning stranger, good morning.\n",
      "\n",
      "1: Hi, how are you? I won't say any more than that.\n",
      "\n",
      "2: Hi, how are you? Good morning everyone!\n",
      "\n",
      "3: Hi, how are you? Good morning all.\n",
      "\n",
      "4: Hi, how are you? to : start listening to your favourite band, do some mushrooms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fae571d5-9419-4f1d-adb1-fa173e3f669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Goodbye, I have seen you around\n",
      "\n",
      "1: Hi, how are you? Zwei Zwei\n",
      "\n",
      "2: Hi, how are you? hello everyone\n",
      "\n",
      "3: Hi, how are you? Good morning! Hello!\n",
      "\n",
      "4: Hi, how are you? hello kesh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=25,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433de377-f772-4350-832b-1c73432ab2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? AMA\n",
      "\n",
      "1: Hi, how are you?, hi\n",
      "\n",
      "2: Hi, how are you? Hello Hello\n",
      "\n",
      "3: Hi, how are you?..\n",
      "\n",
      "4: Hi, how are you?. AMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    top_p=0.92,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be3c4db6-7bb4-4e6a-a041-309f36e4dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_text_samples(model, tokenizer, input_text, device, n_samples = 5):\n",
    "    text_ids = tokenizer.encode(input_text, return_tensors = 'pt')\n",
    "    text_ids = text_ids.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    generated_text_samples = model.generate(\n",
    "        text_ids, \n",
    "        max_length= 100,  \n",
    "        num_return_sequences= n_samples,\n",
    "        no_repeat_ngram_size= 2,\n",
    "        repetition_penalty= 1.5,\n",
    "        top_p= 0.92,\n",
    "        temperature= .85,\n",
    "        do_sample= True,\n",
    "        top_k= 125,\n",
    "        early_stopping= True\n",
    "    )\n",
    "    gen_text = []\n",
    "    for t in generated_text_samples:\n",
    "        text = tokenizer.decode(t, skip_special_tokens=True)\n",
    "        gen_text.append(text)\n",
    "\n",
    "        return gen_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2753c1-7f96-4d54-93d7-9734f52215f1",
   "metadata": {},
   "source": [
    "## Conversational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a14b1-0c9c-4de3-a376-11dd2d5c8058",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "https://huggingface.co/microsoft/DialoGPT-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0166434-a3c2-48c2-8851-67ef68f1c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(text, chat_history_ids=None, step=0):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = base_tokenizer.encode(text + base_tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = base_model.generate(bot_input_ids, max_length=1000, pad_token_id=base_tokenizer.eos_token_id)\n",
    "    \n",
    "    response = base_tokenizer.decode(\n",
    "        chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return response, chat_history_ids, step + 1\n",
    "\n",
    "def chat():\n",
    "    step = 0\n",
    "    chat_history_ids = []\n",
    "    \n",
    "    while True: \n",
    "        text = input(\">> \")\n",
    "        if text in [\"/q\", \"/quit\", \"/e\", \"/exit\"]: break\n",
    "        print(f\"User: {text}\")\n",
    "        response, chat_history_ids, step = generate_responses(\n",
    "            text,\n",
    "            chat_history_ids=chat_history_ids,\n",
    "            step=step\n",
    "        )\n",
    "        print(f\"Bot: {response}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732e590-b732-473c-81b0-e04573f93ac2",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d768c228-6c11-40f1-b726-fa79f36735f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>  /quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef9899-32f8-4f89-8917-53db3c6f6155",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "643ea45d-7857-496d-8a2e-595ef714b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "from typing import Dict, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    data_or_filename: Union[str, pd.DataFrame],\n",
    "    filter_by: str = None,\n",
    "    filter_value: str = None,\n",
    "    content_key: str = \"text\",\n",
    "    n: int = 7,\n",
    "    test_size: float = 0.1,\n",
    "    flatten: bool = True,\n",
    "):\n",
    "    data = load_csv(data_or_filename) if isinstance(data_or_filename, str) else data_or_filename\n",
    "\n",
    "    contexted_data = prepare_context(\n",
    "        data,\n",
    "        filter_by=filter_by,\n",
    "        filter_value=filter_value,\n",
    "        content_key=content_key,\n",
    "        n=n,\n",
    "    )\n",
    "\n",
    "    trn_df, val_df = train_test_split(contexted_data, test_size=test_size)\n",
    "    \n",
    "    #train_dataset = Dataset.from_pandas(trn_df)\n",
    "    #val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    train_dataset = prepare_dataset(trn_df)\n",
    "    val_dataset = prepare_dataset(val_df)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def prepare_context(\n",
    "    data: pd.DataFrame,\n",
    "    filter_by: str = None,\n",
    "    filter_value: str = None,\n",
    "    content_key: str = \"text\",\n",
    "    n: int = 7,\n",
    "):\n",
    "    if filter_by:\n",
    "        indexes = data.loc[data[filter_by] == filter_value].index\n",
    "    else:\n",
    "        indexes = range(n, len(data[content_key]))\n",
    "\n",
    "    contexted = []\n",
    "\n",
    "    for i in indexes:\n",
    "        row = []\n",
    "        prev = i - 1 - n\n",
    "        for j in range(i, prev, -1):\n",
    "            row.append(data.iloc[j][content_key])\n",
    "        contexted.append(row)\n",
    "\n",
    "    columns = [\"response\", \"context\"]\n",
    "    columns = columns + [\"context/\" + str(i) for i in range(n - 1)]\n",
    "\n",
    "    df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    columns = [col for col in df] \n",
    "    dataset = Dataset.from_pandas(concat_text(df))\n",
    "    dataset = dataset.remove_columns(columns + ['__index_level_0__'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def concat_text(df):\n",
    "    df[\"text\"] = df.apply(concat_text_in_row, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_text_in_row(row):\n",
    "    concat_text = f\"{base_tokenizer.eos_token}\".join(row.values)\n",
    "    # Add to end\n",
    "    concat_text += base_tokenizer.eos_token\n",
    "    return concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce5db7f6-8486-4b76-a76f-63559628fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = prepare_data(df, filter_by=\"character\", filter_value=\"bitjockey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baa60833-405c-41dc-8520-66dbdebbd0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4967\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b87bf317-8290-408c-92df-d1342ebada95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 552\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d6c2fc6-d0ef-492a-8520-ca0c344e87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer.pad_token = \"<|PAD|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4919cb3-b1f2-48a4-90b4-e96778c1b0fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda33d9995b44e419fa5b9d6b57984bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4967 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46efbd81afe3410d9039e37171b31998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_ids = base_tokenizer(examples[\"text\"], padding=\"max_length\")[\"input_ids\"]\n",
    "    return {\"input_ids\": input_ids}\n",
    "    \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    input_ids = list(map(tokenize_function, examples))\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "    \n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d081ecb9-c0d5-4f44-9268-ad983ce408ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4967"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52d02923-9740-413b-ba81-c46597c56423",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"perplexity\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52086282-7820-4e5f-91b3-549529ed7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_dialogpt'\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=base_tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "trainer = None\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=model_path,            # directory for storing logs\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e9eab83-e5f6-4da7-8d90-69d808da13c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985db8b4-0d4d-4c9b-a96c-a712c4c1ebf0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4967\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='14901' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  313/14901 01:09 < 54:18, 4.48 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1560\u001b[0m ):\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a77399-5118-458b-8147-54139ede96f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07ecbfd206f94aa49c519ce2b0e792e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1311aa5a50e24e25bba46dbf68f21f69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c1250c960ff414ebb279572b5b1a82d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "35544ad9188542d281c9ced9895d2652": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "39cc37c636fc43e8bcdf4091ddf9b75f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "46efbd81afe3410d9039e37171b31998": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b5c743216f464d219c553f19b77e7576",
        "IPY_MODEL_f3bb23108aec407cb4ede57850d9c449",
        "IPY_MODEL_4d0ae1f42570411d8de57c5167fd2dc0"
       ],
       "layout": "IPY_MODEL_c1142e4677a348b3bf6e537f5e6efc19"
      }
     },
     "480d4c879c794526b090201bb6e4de2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4880244e20b44471a7816391bbd65f35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d0ae1f42570411d8de57c5167fd2dc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1311aa5a50e24e25bba46dbf68f21f69",
       "style": "IPY_MODEL_c4091e4c44a043399ed41e8833c4154f",
       "value": " 552/552 [00:00&lt;00:00, 2546.28ex/s]"
      }
     },
     "82456b6c97cc4806a1a843f14205936a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8740ebe944444a35882cbb52a06d116c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9c55123d9fae48af8a3227d49edde0f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ee3b04a4858e482db1a5926134467b50",
       "style": "IPY_MODEL_e3f2215a22f94d0cbc6406d56b5568a7",
       "value": "100%"
      }
     },
     "adaa17ddb95347b6b9783b05c3fd035c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_480d4c879c794526b090201bb6e4de2c",
       "style": "IPY_MODEL_82456b6c97cc4806a1a843f14205936a",
       "value": " 4967/4967 [00:02&lt;00:00, 2394.51ex/s]"
      }
     },
     "b5c743216f464d219c553f19b77e7576": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_35544ad9188542d281c9ced9895d2652",
       "style": "IPY_MODEL_8740ebe944444a35882cbb52a06d116c",
       "value": "100%"
      }
     },
     "bda33d9995b44e419fa5b9d6b57984bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9c55123d9fae48af8a3227d49edde0f4",
        "IPY_MODEL_f45693d12b1b490087ceac45c5c08529",
        "IPY_MODEL_adaa17ddb95347b6b9783b05c3fd035c"
       ],
       "layout": "IPY_MODEL_edb7a187a7084cc5a8275baa8f5d7117"
      }
     },
     "c1142e4677a348b3bf6e537f5e6efc19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c4091e4c44a043399ed41e8833c4154f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e3f2215a22f94d0cbc6406d56b5568a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "edb7a187a7084cc5a8275baa8f5d7117": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee3b04a4858e482db1a5926134467b50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3bb23108aec407cb4ede57850d9c449": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_07ecbfd206f94aa49c519ce2b0e792e2",
       "max": 552,
       "style": "IPY_MODEL_39cc37c636fc43e8bcdf4091ddf9b75f",
       "value": 552
      }
     },
     "f45693d12b1b490087ceac45c5c08529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4880244e20b44471a7816391bbd65f35",
       "max": 4967,
       "style": "IPY_MODEL_1c1250c960ff414ebb279572b5b1a82d",
       "value": 4967
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
