{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc19e10-b8f3-4b0e-8e0e-6a3d74c79d82",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT\n",
    "\n",
    "https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/#data-loading-and-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5342b571-99e6-4257-9052-9a2eeb9db178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50890486-20bc-4265-b21b-2bc52142159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, re, pandas as pd, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from datasets import Dataset, list_metrics, load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d1459b-99ae-4d54-ae62-1ff1f60666ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"gpt2\"\n",
    "#model_cls = GPT2LMHeadModel\n",
    "#tokenizer_cls = GPT2Tokenizer\n",
    "\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"\n",
    "model_cls = AutoModelForCausalLM\n",
    "tokenizer_cls = AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1722bf0b-9462-4fbe-a369-ab4e3ac13c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    cuda: True\n",
      "    current_device: 0\n",
      "    device_count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev) \n",
    "\n",
    "print(f\"\"\"\n",
    "    cuda: {torch.cuda.is_available()}\n",
    "    current_device: {torch.cuda.current_device()}\n",
    "    device_count: {torch.cuda.device_count()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e5c7d-99cd-4fb0-b92c-c06fc8d31739",
   "metadata": {},
   "source": [
    "## Load Data, Tokenizer, and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae2ca3-2502-4c89-b8f7-d2438099d78b",
   "metadata": {},
   "source": [
    "### Data Loading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6348c3-7b02-48e6-bd03-8b54763c722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32080</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>What're the odds, right? But yes, we, uh...know each other.  And he's also here in Boston.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32081</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Right. Geeze. I...did not expect to ever hear from him again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32082</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>...Surprise?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32083</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>So, uh, would you want to talk to him? Because this is all stuff he really should get the chance to ask you about directly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32084</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Um...sure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32085</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Ok, good. Um, we should probably go. But, uh, I'll see you tomorrow?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32086</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[_as she talks she takes the book off the table and carefully tucks it away in her bag_]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32087</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Um, yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32088</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32089</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[_she will leave with the others_]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       character  \\\n",
       "32080  Charlotte   \n",
       "32081       Toby   \n",
       "32082  Charlotte   \n",
       "32083  Charlotte   \n",
       "32084       Toby   \n",
       "32085  Charlotte   \n",
       "32086  Charlotte   \n",
       "32087       Toby   \n",
       "32088       Toby   \n",
       "32089  Charlotte   \n",
       "\n",
       "                                                                                                                              text  \n",
       "32080                                   What're the odds, right? But yes, we, uh...know each other.  And he's also here in Boston.  \n",
       "32081                                                                Right. Geeze. I...did not expect to ever hear from him again.  \n",
       "32082                                                                                                                 ...Surprise?  \n",
       "32083  So, uh, would you want to talk to him? Because this is all stuff he really should get the chance to ask you about directly.  \n",
       "32084                                                                                                                   Um...sure.  \n",
       "32085                                                         Ok, good. Um, we should probably go. But, uh, I'll see you tomorrow?  \n",
       "32086                                     [_as she talks she takes the book off the table and carefully tucks it away in her bag_]  \n",
       "32087                                                                                                                    Um, yeah.  \n",
       "32088                                                                                                                    Tomorrow.  \n",
       "32089                                                                                           [_she will leave with the others_]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"../data/processed.csv\"\n",
    "df = pd.read_csv(filepath, encoding=\"utf-8\", usecols=[\"character\", \"content\"]).rename(columns={\"content\": \"text\"})\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f9585-6e12-4bf1-990f-40aeb39f5952",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b09bb6c-1983-4e87-b805-3dab99ae9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model_cls.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502b3b5d-d0b6-47f9-9424-a0a646a95ba0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ModuleUtilsMixin.num_parameters of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e23c7b-052d-411c-ac45-66bfcf9540f0",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ff1d2c-2dfd-4f29-939e-bf4f299a3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = tokenizer_cls.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138c94af-7007-4dbb-afe4-59eeeb05d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab_size: {base_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfce691-d927-41cd-95bd-db815dfb6c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = base_tokenizer.get_vocab()\n",
    "vocabulary[\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affbeb1a-faed-4cae-9c70-be9de39dc1e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode\n",
    "text = \"Hi, how are you?\"\n",
    "base_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fb3dea-2b04-482d-913b-2289a6089cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17250,    11,   703,   389,   345,    30]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids = base_tokenizer.encode(text, return_tensors = 'pt')\n",
    "text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9ad140-f825-499d-b16e-03e7ec09cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ae1eac-5b6b-4435-82f4-a137481220a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a1ff8-a41b-40f1-a5a9-4fb6211b04a9",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944c7d1-207e-4a11-b959-54a18f3bd1a6",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637843c6-cac9-44dc-bc6c-866c7a88d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ejemplo de generación de texto\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 100,\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcadd20-f8ff-420f-a77f-59003a8b8112",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1501ac8-53f3-442c-994b-b37d2c427c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n",
      "1: Hi, how are you? Good morning!\n",
      "\n",
      "2: Hi, how are you?'s hello\n",
      "\n",
      "3: Hi, how are you? Good Morning everyone!\n",
      "\n",
      "4: Hi, how are you? Good morning everyone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    num_return_sequences= 5,\n",
    "    early_stopping=True \n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873ed406-4987-4167-bf11-259abcadc2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good morning everyone!\n",
      "\n",
      "1: Hi, how are you? Good morning!\n",
      "\n",
      "2: Hi, how are you?'s hello\n",
      "\n",
      "3: Hi, how are you? Good Morning everyone!\n",
      "\n",
      "4: Hi, how are you? Good morning everyone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences= 5,\n",
    "    early_stopping=True \n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "854fe608-e368-4bd7-b428-4efa2d6086dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good luck with all your studies.\n",
      "\n",
      "1: Hi, how are you? poem aloud\n",
      "\n",
      "2: Hi, how are you?\n",
      "\n",
      "3: Hi, how are you? hello\n",
      "\n",
      "4: Hi, how are you? Good morning sun beetle lover. uk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02e0afa2-fa35-493b-b473-897388a3665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? hello how are you hi how long.\n",
      "\n",
      "1: Hi, how are you? Today, I think we've all seen the infamous Dacia Sandero and how one of the Others have been round quite a bit lately.\n",
      "\n",
      "2: Hi, how are you? Friend request.\n",
      "\n",
      "3: Hi, how are you? Hello\n",
      "\n",
      "4: Hi, how are you?'s hello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613fef48-67a7-4303-8e17-eb82107b0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Good Morning America here from USA.\n",
      "\n",
      "1: Hi, how are you?.\n",
      "\n",
      "2: Hi, how are you?, : Are you fed up, dear?\n",
      "\n",
      "3: Hi, how are you? Hi I'm Beardies, only lvl 20 hunter.\n",
      "\n",
      "4: Hi, how are you? Good morning!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fae571d5-9419-4f1d-adb1-fa173e3f669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? After all the amazing news you've been having lately hello!\n",
      "\n",
      "1: Hi, how are you? hello how are you\n",
      "\n",
      "2: Hi, how are you? Good Morning, good morning, good morning good morning good morning good morning good morning...\n",
      "\n",
      "3: Hi, how are you? Good morning everyone! Hope everyone has great days.\n",
      "\n",
      "4: Hi, how are you?..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=25,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433de377-f772-4350-832b-1c73432ab2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hi, how are you? Im A. A. Rodriguez...\n",
      "\n",
      "1: Hi, how are you? Good morning everyone, everyone.\n",
      "\n",
      "2: Hi, how are you?.\n",
      "\n",
      "3: Hi, how are you? Good morning!\n",
      "\n",
      "4: Hi, how are you? I got famous... your famous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text generation example\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    do_sample=True,  \n",
    "    top_k=0,\n",
    "    top_p=0.92,\n",
    "    num_return_sequences= 5\n",
    ")\n",
    "\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be3c4db6-7bb4-4e6a-a041-309f36e4dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_text_samples(model, tokenizer, input_text, device, n_samples = 5):\n",
    "    text_ids = tokenizer.encode(input_text, return_tensors = 'pt')\n",
    "    text_ids = text_ids.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    generated_text_samples = model.generate(\n",
    "        text_ids, \n",
    "        max_length= 100,  \n",
    "        num_return_sequences= n_samples,\n",
    "        no_repeat_ngram_size= 2,\n",
    "        repetition_penalty= 1.5,\n",
    "        top_p= 0.92,\n",
    "        temperature= .85,\n",
    "        do_sample= True,\n",
    "        top_k= 125,\n",
    "        early_stopping= True\n",
    "    )\n",
    "    gen_text = []\n",
    "    for t in generated_text_samples:\n",
    "        text = tokenizer.decode(t, skip_special_tokens=True)\n",
    "        gen_text.append(text)\n",
    "\n",
    "        return gen_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2753c1-7f96-4d54-93d7-9734f52215f1",
   "metadata": {},
   "source": [
    "## Conversational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a14b1-0c9c-4de3-a376-11dd2d5c8058",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "https://huggingface.co/microsoft/DialoGPT-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0166434-a3c2-48c2-8851-67ef68f1c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, text, chat_history_ids=None, step=0):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return response, chat_history_ids, step + 1\n",
    "\n",
    "def chat(model, tokenizer):\n",
    "    step = 0\n",
    "    chat_history_ids = []\n",
    "    \n",
    "    while True: \n",
    "        text = input(\">> \")\n",
    "        if text in [\"/q\", \"/quit\", \"/e\", \"/exit\"]: break\n",
    "        print(f\"User: {text}\")\n",
    "        response, chat_history_ids, step = generate_responses(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            text=text,\n",
    "            chat_history_ids=chat_history_ids,\n",
    "            step=step\n",
    "        )\n",
    "        print(f\"Bot: {response}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732e590-b732-473c-81b0-e04573f93ac2",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d768c228-6c11-40f1-b726-fa79f36735f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef9899-32f8-4f89-8917-53db3c6f6155",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "643ea45d-7857-496d-8a2e-595ef714b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "from typing import Dict, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    data_or_filename: Union[str, pd.DataFrame],\n",
    "    filter_by: str = None,\n",
    "    filter_value: str = None,\n",
    "    content_key: str = \"text\",\n",
    "    n: int = 7,\n",
    "    test_size: float = 0.1,\n",
    "    flatten: bool = True,\n",
    "):\n",
    "    data = load_csv(data_or_filename) if isinstance(data_or_filename, str) else data_or_filename\n",
    "\n",
    "    contexted_data = prepare_context(\n",
    "        data,\n",
    "        filter_by=filter_by,\n",
    "        filter_value=filter_value,\n",
    "        content_key=content_key,\n",
    "        n=n,\n",
    "    )\n",
    "\n",
    "    trn_df, val_df = train_test_split(contexted_data, test_size=test_size)\n",
    "    \n",
    "    #train_dataset = Dataset.from_pandas(trn_df)\n",
    "    #val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    train_dataset = prepare_dataset(trn_df)\n",
    "    val_dataset = prepare_dataset(val_df)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def prepare_context(\n",
    "    data: pd.DataFrame,\n",
    "    filter_by: str = None,\n",
    "    filter_value: str = None,\n",
    "    content_key: str = \"text\",\n",
    "    n: int = 7,\n",
    "):\n",
    "    if filter_by:\n",
    "        indexes = data.loc[data[filter_by] == filter_value].index\n",
    "    else:\n",
    "        indexes = range(n, len(data[content_key]))\n",
    "\n",
    "    contexted = []\n",
    "\n",
    "    for i in indexes:\n",
    "        row = []\n",
    "        prev = i - 1 - n\n",
    "        for j in range(i, prev, -1):\n",
    "            row.append(data.iloc[j][content_key])\n",
    "        contexted.append(row)\n",
    "\n",
    "    columns = [\"response\", \"context\"]\n",
    "    columns = columns + [\"context/\" + str(i) for i in range(n - 1)]\n",
    "\n",
    "    df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    columns = [col for col in df] \n",
    "    dataset = Dataset.from_pandas(concat_text(df))\n",
    "    dataset = dataset.remove_columns(columns + ['__index_level_0__'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def concat_text(df):\n",
    "    df[\"text\"] = df.apply(concat_text_in_row, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_text_in_row(row):\n",
    "    concat_text = f\"{base_tokenizer.eos_token}\".join(row.values)\n",
    "    # Add to end\n",
    "    concat_text += base_tokenizer.eos_token\n",
    "    return concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce5db7f6-8486-4b76-a76f-63559628fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = prepare_data(df, filter_by=\"character\", filter_value=\"bitjockey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baa60833-405c-41dc-8520-66dbdebbd0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4967\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b87bf317-8290-408c-92df-d1342ebada95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 552\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d6c2fc6-d0ef-492a-8520-ca0c344e87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer.pad_token = \"<|PAD|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4919cb3-b1f2-48a4-90b4-e96778c1b0fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535b406cedf74e409bccb51de2c71bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4967 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6939bc063b244e0bbb87387ec46c026f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_ids = base_tokenizer(examples[\"text\"], padding=\"max_length\", max_length=500)[\"input_ids\"]\n",
    "    return {\"input_ids\": input_ids}\n",
    "    \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    input_ids = list(map(tokenize_function, examples))\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "    \n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d081ecb9-c0d5-4f44-9268-ad983ce408ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4967"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee5835-da48-468b-b2af-45d407d9a8c9",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52d02923-9740-413b-ba81-c46597c56423",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"perplexity\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2313d34-3806-4d23-a40a-495d36c084e2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52086282-7820-4e5f-91b3-549529ed7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_dialogpt'\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=base_tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "trainer = None\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=model_path,            # directory for storing logs\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e9eab83-e5f6-4da7-8d90-69d808da13c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985db8b4-0d4d-4c9b-a96c-a712c4c1ebf0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4967\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7452' max='7452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7452/7452 25:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.224500</td>\n",
       "      <td>2.913335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.756600</td>\n",
       "      <td>2.656801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.508000</td>\n",
       "      <td>2.578112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_dialogpt/checkpoint-500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "4.4305\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.6645195920558245e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-1000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "3.5681\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.329039184111648e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-1500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "3.376\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.993558776167472e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-2000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "3.2245\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.6580783682232964e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 552\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "2.913335084915161\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "15.5717\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "35.449\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "17.724\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_dialogpt/checkpoint-2500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "3.1537\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.32259796027912e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-3000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.8394\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.9871175523349438e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-3500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.8516\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.6516371443907677e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-4000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.806\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.3161567364465916e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-4500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.7566\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.9806763285024154e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-4500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 552\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "2.656801462173462\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "15.5647\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "35.465\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "17.732\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_dialogpt/checkpoint-5000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.7056\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.6451959205582396e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-5500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.5593\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.3097155126140634e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-6000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-6000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.5338\n",
      "Attempted to log scalar metric learning_rate:\n",
      "9.742351046698874e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-6500\n",
      "Configuration saved in ./model_dialogpt/checkpoint-6500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.5226\n",
      "Attempted to log scalar metric learning_rate:\n",
      "6.3875469672571135e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./model_dialogpt/checkpoint-7000\n",
      "Configuration saved in ./model_dialogpt/checkpoint-7000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "2.508\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.032742887815352e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_dialogpt/checkpoint-7000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 552\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "2.5781116485595703\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "15.5606\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "35.474\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "17.737\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric train_runtime:\n",
      "1549.4789\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "9.617\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "4.809\n",
      "Attempted to log scalar metric total_flos:\n",
      "3802258368000000.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "2.960053423944495\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7452, training_loss=2.960053423944495, metrics={'train_runtime': 1549.4789, 'train_samples_per_second': 9.617, 'train_steps_per_second': 4.809, 'total_flos': 3802258368000000.0, 'train_loss': 2.960053423944495, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0a77399-5118-458b-8147-54139ede96f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model_dialogpt\n",
      "Configuration saved in model_dialogpt/config.json\n",
      "Model weights saved in model_dialogpt/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"model_dialogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490541df-a595-4c80-8e5c-9b8ff66a9c39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"model_dialogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6daae3-1241-4782-920e-16b5e67933f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, text, chat_history_ids=None, step=0):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return response, chat_history_ids, step + 1\n",
    "\n",
    "def chat(model, tokenizer):\n",
    "    step = 0\n",
    "    chat_history_ids = []\n",
    "    \n",
    "    while True: \n",
    "        text = input(\">> \")\n",
    "        if text in [\"/q\", \"/quit\", \"/e\", \"/exit\"]: break\n",
    "        print(f\"User: {text}\")\n",
    "        response, chat_history_ids, step = generate_responses(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            text=text,\n",
    "            chat_history_ids=chat_history_ids,\n",
    "            step=step\n",
    "        )\n",
    "        print(f\"Bot: {response}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e5dce-153c-46be-80b7-f29db173c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n",
      "Bot: I'm back!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>  How are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How are you\n",
      "Bot: I'm here!\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "chat(base_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e68c67-fb17-4ef0-9d0b-757410419442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
